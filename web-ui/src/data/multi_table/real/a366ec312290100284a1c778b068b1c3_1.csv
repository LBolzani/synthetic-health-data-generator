,Paper title,Abstract summary,Authors,Journal,Influential citations,DOI,PDF,Year,Citations,DOI URL,Semantic Scholar URL,Abstract,Takeaway suggests yes/no,Study type,Number of participants,Intervention,Outcomes measured
0,SIMPL - A Framework for Accessing External Data in Simulation Workflows,SIMPL removes the burden from engineers and scientists to specify low-level details of data management for their simulation applications.,"P. Reimann, Michael Reiter, H. Schwarz, Dimka Karastoyanova, F. Leymann",BTW,,,,2011,38.0,,https://semanticscholar.org/paper/e874b91b808ff0d186631d9c9fc847aa40197624,"Adequate data management and data provisioning are among the most important topics to cope with the information explosion intrinsically associated with simulation applications. Today, data exchange with and between simulation applications is mainly accomplished in a file-style manner. These files show proprietary formats and have to be transformed according to the specific needs of simulation applications. Lots of effort has to be spent to find appropriate data sources and to specify and implement data transformations. In this paper, we present SIMPL – an extensible framework that provides a generic and consolidated abstraction for data management and data provisioning in simulation workflows. We introduce extensions to workflow languages and show how they are used to model the data provisioning for simulation workflows based on data management patterns. Furthermore, we show how the framework supports a uniform access to arbitrary external data in such workflows. This removes the burden from engineers and scientists to specify low-level details of data management for their simulation applications and thus boosts their productivity.",,,,,
1,Multi-infrastructure workflow execution for medical simulation in the Virtual Imaging Platform,The Virtual Imaging Platform can execute medical image simulation workflows on multiple computing infrastructures.,"Rafael Ferreira da Silva, S. Camarasu-Pop, Baptiste Grenier, V. Hamar, D. Manset, J. Montagnat, J. Revillard, J. R. Balderrama, A. Tsaregorodtsev, T. Glatard",,,,,2011,24.0,,https://semanticscholar.org/paper/36059e6107e83039e6fa8d44c9642effe3199c0c,This paper presents the architecture of the Virtual Imaging Platform sup- porting the execution of medical image simulation workflows on multiple comput- ing infrastructures. The system relies on the MOTEUR engine for workflow execu- tion and on the DIRAC pilot-job system for workload management. The jGASW code wrapper is extended to describe applications running on multiple infrastruc- tures and a DIRAC cluster agent that can securely involve personal cluster re- sources with no administrator intervention is proposed. Grid data management is complemented with local storage used as a failover in case of file transfer errors. Between November 2010 and April 2011 the platform was used by 10 users to run 484 workflow instances representing 10.8 CPU years. Tests show that a small per- sonal cluster can significantly contribute to a simulation running on EGI and that the improved data manager can decrease the job failure rate from 7.7% to 1.5%.,,,10.0,Virtual Imaging Platform,"• architecture of the Virtual Imaging Platform
• the MOTEUR engine for workflow execu  tion
• the DIRAC pilot job system for workload management
• the jGASW code wrapper
• a DIRAC cluster agent
• Grid data management
• local storage"
2,Optimizing bioinformatics workflows for data analysis using cloud management techniques,Cloud resource monitoring and management strategies are required to determine the resource consumption behaviors of workflow applications.,"Vincent C. Emeakaroha, Pawel P. Labaj, M. Maurer, I. Brandić, David P. Kreil",WORKS '11,,10.1145/2110497.2110503,http://www.infosys.tuwien.ac.at/staff/ivona/papers/BioInformaticWorkflows.pdf,2011,17.0,https://doi.org/10.1145/2110497.2110503,https://semanticscholar.org/paper/6d75331f23f80f650b4ba17f43514c80998f0b38,"With the rapid development in recent years of high-throughput technologies in the life sciences, huge amounts of data are being generated and stored in databases. Despite significant advances in computing capacity and performance, an analysis of these large-scale data in a search for biomedically relevant patterns remains a challenging task. Scientific workflow applications support data-mining in more complex scenarios that include many data sources and computational tools, as commonly found in bioinformatics. A scientific workflow application is a holistic unit that defines, executes, and manages scientific applications using different software tools. Existing workflow applications are process- or data- rather than resource-oriented. Thus, they lack efficient computational resource management capabilities, such as those provided by Cloud computing environments. Insufficient computational resources disrupt the execution of workflow applications, wasting time and money. To address this issue, advanced resource monitoring and management strategies are required to determine the resource consumption behaviours of workflow applications for a dynamical allocation and deallocation of resources. In this paper, we present a novel Cloud resource monitoring technique and a knowledge management strategy to manage computational resources for workflow applications in order to guarantee their performance goals and their successful completion. We present the design description of these techniques, demonstrate how they can be applied to scientific workflow applications, and present first evaluation results as a proof of concept.",,,,,
3,In‐memory staging and data‐centric task placement for coupled scientific simulation workflows,In-memory data staging and data-centric task placement can reduce the cost of data movement in coupled scientific simulation workflows.,"Fan Zhang, Tong Jin, Qian Sun, Melissa Romanus, H. Bui, S. Klasky, M. Parashar",Concurr. Comput. Pract. Exp.,,10.1002/cpe.4147,,2017,14.0,https://doi.org/10.1002/cpe.4147,https://semanticscholar.org/paper/ab0f99f344b136d3d57e7d8c669f78e8147ace48,"Coupled scientific simulation workflows are composed of heterogeneous component applications that simulate different aspects of the physical phenomena being modeled and that interact and exchange significant volumes of data at runtime. As the data volumes and generation rates keep growing, the traditional disk I/O–based data movement approach becomes cost prohibitive, and workflow requires more scalable and efficient approach to support the data movement. Moreover, the cost of moving large volume of data over system interconnection network becomes dominating and significantly impacts the workflow execution time. Minimize the amount of network data movement and localize data transfers are critical for reducing such cost. To achieve this, workflow task placement should exploit data locality to the extent possible and move computation closer to data. In this paper, we investigate applying in‐memory data staging and data‐centric task placement to reduce the data movement cost in large‐scale coupled simulation workflows. Specifically, we present a distributed data sharing and task execution framework that (1) co‐locates in‐memory data staging on application compute nodes to store data that needs to be shared or exchanged and (2) uses data‐centric task placement to map computations onto processor cores that a large portion of the data exchanges can be performed using the intra‐node shared memory. We also present the implementation of the framework and its experimental evaluation on Titan Cray XK7 petascale supercomputer.",,,,in memory data staging and data centric task placement,
4,A Pattern Approach to Conquer the Data Complexity in Simulation Workflow Design,Abstract uses words related to scientific workflows and data management.,"Peter  Reimann, Holger  Schwarz, Bernhard  Mitschang",OTM Conferences,,10.1007/978-3-662-45563-0_2,,2014,9.0,https://doi.org/10.1007/978-3-662-45563-0_2,https://semanticscholar.org/paper/591bb9077d2988af3fa567086926d5ddfa5c7a38,"Scientific workflows may be used to enable the collaborative implementation of scientific applications across various domains. Since each domain has its own requirements and solutions for data handling, such workflows often have to deal with a highly heterogeneous data environment. This results in an increased complexity of workflow design. As scientists typically design their scientific workflows on their own, this complexity hinders them to concentrate on their core issue, namely the experiments, analyses, or simulations they conduct. In this paper, we present a novel approach to a pattern-based abstraction support for the complex data management in simulation workflows that goes beyond related work in similar research areas. A pattern hierarchy with different abstraction levels enables a separation of concerns according to the skills of different persons involved in workflow design. The goal is that scientists are no longer obliged to specify low-level details of data management in their workflows. We discuss the advantages of this approach and show to what extent it reduces the complexity of simulation workflow design. Furthermore, we illustrate how to map patterns onto executable workflows. Based on a prototypical implementation of three real-world simulations, we evaluate our approach according to relevant requirements.",,,,,
5,Simulation environment and graphical visualization environment: a COPD use-case,A simulation environment allows researchers to research and study the internal mechanisms of the human physiology.,"Mercedes  Huertas-Migueláñez, Daniel  Mora, Isaac  Cano, Dieter  Maier, David  Gomez-Cabrero, Magí  Lluch-Ariet, Felip  Miralles",Journal of Translational Medicine,,10.1186/1479-5876-12-S2-S7,,2014,8.0,https://doi.org/10.1186/1479-5876-12-S2-S7,https://semanticscholar.org/paper/10c0946c7bb7fa83665a5152a10513fe01582966,"BackgroundToday, many different tools are developed to execute and visualize physiological models that represent the human physiology. Most of these tools run models written in very specific programming languages which in turn simplify the communication among models. Nevertheless, not all of these tools are able to run models written in different programming languages. In addition, interoperability between such models remains an unresolved issue.ResultsIn this paper we present a simulation environment that allows, first, the execution of models developed in different programming languages and second the communication of parameters to interconnect these models. This simulation environment, developed within the Synergy-COPD project, aims at helping and supporting bio-researchers and medical students understand the internal mechanisms of the human body through the use of physiological models. This tool is composed of a graphical visualization environment, which is a web interface through which the user can interact with the models, and a simulation workflow management system composed of a control module and a data warehouse manager. The control module monitors the correct functioning of the whole system. The data warehouse manager is responsible for managing the stored information and supporting its flow among the different modules.This simulation environment has been validated with the integration of three models: two deterministic, i.e. based on linear and differential equations, and one probabilistic, i.e., based on probability theory. These models have been selected based on the disease under study in this project, i.e., chronic obstructive pulmonary disease.ConclusionIt has been proved that the simulation environment presented here allows the user to research and study the internal mechanisms of the human physiology by the use of models via a graphical visualization environment. A new tool for bio-researchers is ready for deployment in various use cases scenarios.",,,,,
6,Towards Data and Data Quality Management for Large Scale Healthcare Simulations - Position Paper,The approach of ProHTA is to understand the impact of medical processes and technologies as early as possible.,"P. Baumgärtel, R. Lenz",HEALTHINF,,10.5220/0003871602750280,,2012,7.0,https://doi.org/10.5220/0003871602750280,https://semanticscholar.org/paper/2d0a239bd6e61d0032a73c29a166f89c9e123bf7,"The approach of ProHTA (Prospective Health Technology Assessment) is to understand the impact of medical processes and technologies as early as possible. Therefore, simulation techniques are utilized to estimate the effects of innovative health technologies and find potentials of efficiency enhancement within the supply chain of healthcare. Data management for healthcare simulations is required as heterogeneous data is needed both as simulation input data and for validation purposes. The main problem is the heterogeneity of the data and the initially unknown and continuously changing demands of the simulation. Also, data quality considerations are necessary to quantify the reliability of simulation output. A solution has to consider all of these aspects and must be extensible to cope with changing requirements. As the structure of the data is not known in advance, a generic database schema is required. This paper proposes an approach to store heterogeneous statistical data in an RDF-triplestore. Semantic annotations based on conceptual models are utilized to describe the datasets. Additionally, a special query language helps loading the data into the simulation. The feasibility of the approach has been demonstrated in a prototype implementation. We discuss the benefits of this approach as well as remaining challenges and issues.",,,,,
7,Specification of Simulation Data Management Environment Integrated with PDM,The numerical simulation gradually became one of the industrial tools to reduce the design cycles.,"S. Charles, B. Eynard",,,,,2005,6.0,,https://semanticscholar.org/paper/1e044b39b6af28088350c215a6deb501210c2585,"The numerical simulation gradually became one of the industrial tools to reduce the design cycles while guaranteeing the improvement of the quality and the performances of the products. Current simulation approaches move towards the simultaneous analysis with multiple parameters such as stochastic simulations (statistical), multi-physics analyses or multi-scenarios analyses (for optimization). The direct impacts of these new approaches are the increase in the number of simulations and alternatives of simulation, the growth of the complexity of the simulation models, and therefore, the drastic increase in the volume of data. The handling of an exponential volume of data reinforces the importance to define a consistent simulation-data-management environment to improve the communication, the synchronization and the tractability of the data generated by the numerous loops of design/validation. This environment must guarantee homogeneity of the data (to ensure compatibility), a simple and transparent access, and an effective management of the important volume of data. This paper deals with the development of methods and tools to efficiently manage the simulation data in order to enhance the integration of the finite element analysis and the design activities in a concurrent engineering context.",,,,,
8,Web-Based Distributed Simulation and Data Management Services for Medical Applications,A Web-based medical information system with a Web services technique can execute simulations and access data from/to distributed storage.,"M. Nakagawa, K. Nozaki, S. Shimojo",19th IEEE Symposium on Computer-Based Medical Systems (CBMS'06),,10.1109/CBMS.2006.170,,2006,5.0,https://doi.org/10.1109/CBMS.2006.170,https://semanticscholar.org/paper/5067ba24d1539a30012590d9dbd199071326c9ae,"We proposed a distributed service oriented system with Web-based interface for medical information to promote the practical prediction of clinical prognosis. The proposed system provided hospitals secure access for medical information and processing simulations to examine effects of surgical treatments. Through a Web interface, users could operate the system in order to execute the simulations, access data from/to distributed storage, and display the medical information. These functions were combined by Web services technique, and ontology mapping based on ICD10 was applied to combine various types of medical information. A client-side volume display using GPU was developed to visualize the result volumetric data interactively, and the system adjusted data precision dynamically to maintain the rendering speed. Being based on standardized mechanism, our framework can be applied to similar complex problems with different types of and quantities of information, such as microscopy or geography",,,,,
9,Integration of modern data management practice with scientific workflows,Modern science increasingly involves managing and processing large amounts of distributed data accessed by global teams of researchers.,"N. Killeen, Jason M. Lohrey, M. Farrell, Wilson Liu, S. Garic, D. Abramson, Hoang Anh Nguyen, G. Egan",2012 IEEE 8th International Conference on E-Science,,10.1109/eScience.2012.6404426,,2012,5.0,https://doi.org/10.1109/eScience.2012.6404426,https://semanticscholar.org/paper/624e9cc68ca2b901532f823ba91cfbe896e1e692,"Modern science increasingly involves managing and processing large amounts of distributed data accessed by global teams of researchers. To do this, we need systems that combine data, meta-data and workflows into a single system. This paper discusses such a system, built from a number of existing technologies. We demonstrate the effectiveness on a case study that analyses MRI data.",,,,,
10,"A demonstration of modularity, reuse, reproducibility, portability and scalability for modeling and simulation of cardiac electrophysiology using Kepler Workflows",Multi-scale computational modeling is a major branch of computational biology.,"Pei-Chi Yang, Shweta Purawat, P. Ieong, Mao-Tsuen Jeng, K. DeMarco, I. Vorobyov, A. McCulloch, I. Altintas, R. Amaro, C. Clancy",PLoS Comput. Biol.,,10.1371/journal.pcbi.1006856,,2019,4.0,https://doi.org/10.1371/journal.pcbi.1006856,https://semanticscholar.org/paper/c9d272223499732e395190938ca017c228637062,"Multi-scale computational modeling is a major branch of computational biology as evidenced by the US federal interagency Multi-Scale Modeling Consortium and major international projects. It invariably involves specific and detailed sequences of data analysis and simulation, often with multiple tools and datasets, and the community recognizes improved modularity, reuse, reproducibility, portability and scalability as critical unmet needs in this area. Scientific workflows are a well-recognized strategy for addressing these needs in scientific computing. While there are good examples if the use of scientific workflows in bioinformatics, medical informatics, biomedical imaging and data analysis, there are fewer examples in multi-scale computational modeling in general and cardiac electrophysiology in particular. Cardiac electrophysiology simulation is a mature area of multi-scale computational biology that serves as an excellent use case for developing and testing new scientific workflows. In this article, we develop, describe and test a computational workflow that serves as a proof of concept of a platform for the robust integration and implementation of a reusable and reproducible multi-scale cardiac cell and tissue model that is expandable, modular and portable. The workflow described leverages Python and Kepler-Python actor for plotting and pre/post-processing. During all stages of the workflow design, we rely on freely available open-source tools, to make our workflow freely usable by scientists.",,,,,
11,Towards autonomic data management for staging-based coupled scientific workflows,Adaptive data management is required in scientific workflows to efficiently meet dynamic data requirements.,"Tong  Jin, Fan  Zhang, Qian  Sun, Melissa  Romanus, Hoang  Bui, Manish  Parashar",J. Parallel Distributed Comput.,,10.1016/j.jpdc.2020.07.002,https://www.sciencedirect.com/science/article/am/pii/S0743731520303312,2020,3.0,https://doi.org/10.1016/j.jpdc.2020.07.002,https://semanticscholar.org/paper/45797fda6d7e8c4545f188b90daa37724a74c506,"Abstract Emerging scientific workflows running at extreme scale are composed of multiple applications that interact and exchange data at runtime. While staging-based approaches, e.g. in-situ/in-transit processing, are promising, dynamic behaviors (e.g. data volumes and distributions) in coupled applications and varying resource constraints at runtime make the efficient use of these techniques challenging. Addressing these challenges requires fundamental changes in the way that workflows are executed at runtime. Specifically, it is required to monitor the operating environment and running applications, and then adapt and tune the application behaviors and resource allocations at runtime while meeting the data management requirements and constraints. In this paper, we propose a policy-based autonomic data management (ADM) approach that can adaptively respond at runtime to dynamic data management requirements. We first formulate the schematic abstraction of this ADM approach including its conceptual model and system elements. Then, we explore the realization of ADM runtime and demonstrate how to achieve adaptations in a cross-layer manner with pre-defined autonomic policies. We also prototype our ADM approach and evaluate its performance on the Intrepid IBM-BlueGene and Titan Cray-XK7 systems using Chombo-based AMR applications and a visualization application. The experimental results demonstrate its effectiveness in meeting user defined objectives and accelerating overall scientific discovery.",,,,,
12,Management of simulation studies in computational biology,The rate of reproducibility of scientific results in computational biology is not acceptable.,Dagmar Waltemath,,,,,2015,3.0,,https://semanticscholar.org/paper/a8d533df48969c5f5cda9cd9559cb6c1fb8323bc,"Data management is a well defined task in computer science which investigates methods for organising and controlling the information generated during (research) projects. It comprises several tasks, including data storage, search, retrieval, version control and provenance. Effective data management strategies for computational biology are needed to handle the increasing amount of data that is being generated and processed: High-throughput experiments generate large amounts of data; computational models become complex; novel methods for model coupling enable researchers to combine models into even larger systems; increasing computational power allows for complex simulations; and the availability of data at different scales demands clever integration techniques. However, recent studies showed that the rate of reproducibility of scientific results in the life sciences, including computational biology, is not acceptable [Ioa14]. As a consequence, efforts have been launched to improve reusability and reproducibility of biomedical results (e. g., [M14, Ioa14]), and results of simulation studies in particular [W11a, B14, C15]. Today, paths towards improved data management are discussed by funders and publishers, in large scale projects and by individual researchers. For example, funders established policies such as the ERASysAPP Data Management Guidelines; the German Network for Bioinformatics Infrastructure, de.NBI (http://www.denbi.de) has dedicated data management centers; and projects are funded to develop support for sustainable data management, e. g. , FAIR-DOM (http://fair-dom.org).",,,,,
13,Management in clinical simulation: a proposal for best practices and process optimization.,Creating a document with good practices in simulation regarding management and practice in simulation and management of resources and data.,"Patrícia Ilha Schuelter, F. Tourinho, Vera Radünz, V. E. P. Santos, Vivian Costa Fermo, S.L.E.S Barbosa",Revista brasileira de enfermagem,,10.1590/0034-7167-2020-0515,,2021,2.0,https://doi.org/10.1590/0034-7167-2020-0515,https://semanticscholar.org/paper/d919d2b000761aeb04df58355e8d94cc687d5cdb,"OBJECTIVES

to develop a best practices document with facilitating components and processes for simulation management.

METHODS

the methodological research was conducted between April and October 2017, using four approaches: observational research, conducted in an international simulation institution; Definition of theoretical framework, from the International Nursing Association for Clinical Simulation and Learning; integrative literature review, in international databases; and comparative analysis. It used Bardin's analysis for the categorization of the information.

RESULTS

creation of a document with good practices in simulation regarding management and practice in simulation and management of resources and data, highlighting the use of technology and the training of professionals as the most important allies for overcoming the main limitations found.

FINAL CONSIDERATIONS

the product of this study is a compilation of strategies for simulation management as a tool to enhance the application of the method with greater effectiveness.",,Review,,,
14,The Design of a Computer Simulator to Emulate Pathology Laboratory Workflows,The pathologist laboratory workflow simulator improves the laboratory's efficiency in testing and analysis.,"Megan Patterson, R. Bond, M. Mulvenna, C. Reid, Fiona McMahon, P. McGowan, K. Cowan, H. Cormican",ECCE,,10.1145/2970930.2970956,,2016,1.0,https://doi.org/10.1145/2970930.2970956,https://semanticscholar.org/paper/852ddf587b286f079496747b9238d09e08151f92,"This paper outlines the design of a simulator to allow for the optimisation of clinical workflows within a pathology laboratory and to improve the laboratory's efficiency in the processing, testing, and analysis of specimens. The aim of this research project is to determine whether the simulator can improve clinical workflows since it's design is based on relevant human factors and cognitive ergonomics in mind. Often pathologists have difficulty in pinpointing and anticipating issues in the clinical workflow until tests are running late or in error. It can be difficult to pinpoint the cause and even more difficult to predict any issues which may arise. For example, they often have no indication of how many samples are going to be delivered to the laboratory that day or at a given hour. If we could model scenarios using past information and also the 'live' known variables, it would be possible for pathology laboratories to prepare the appropriate resources, e.g. the printing of specimen labels or to activate a sufficient number of technicians. This would expedite the clinical workload, clinical processes and improve the overall efficiency of the laboratory. The simulator will also be used for the purposes of training new staff on the workflow and practices of the laboratory.",,,,a computer simulator to emulate pathology laboratory workflows,
15,Customized workflow development and omics data integration concepts in systems medicine,The ever-increasing amount and diversity of biological and medical data is a major challenge in computational analyses.,M. Wolfien,,,10.18453/ROSDOK_ID00002868,,2020,,https://doi.org/10.18453/ROSDOK_ID00002868,https://semanticscholar.org/paper/d2e135f0389ec210dc8025ef2ba4fd9df829eefd,"The ever-increasing amount and diversity of biological and medical data is a major challenge in computational analyses. Computational methods have to be combined into analysis workflows for seamless, swift, and transparent computation. In this work, numerous workflows were developed for the general processing of bulk RNA sequencing (RNA-Seq), single-cell sequencing experiments, and non-coding RNA identification. Mathematical concepts of machine learning and univariate meta-analyses were successfully implemented to independently investigate the role of cell therapies in cardiac regeneration.",,,,cell therapies,
